{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4efc2fde-43f3-4659-88ec-524310a1cb5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import GaussianCopulaSynthesizer\n",
    "# from pyspark.sql import SparkSession\n",
    "# from rdt.transformers import FloatFormatter\n",
    "# from rdt.transformers import PseudoAnonymizedFaker \n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sdv\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f84a71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Predictive Model Folder: c:\\Users\\ashly\\OneDrive\\Documents\\Education Material\\ResearchProject\\MaternalHealthResearch\\predictive-model\n",
      "sdv version= 1.10.0\n"
     ]
    }
   ],
   "source": [
    "# set working directory // place commands in .env\n",
    "new_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "os.chdir(new_dir)\n",
    "\n",
    "print('In Predictive Model Folder:', os.getcwd())\n",
    "print('sdv version=',sdv.version.public)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f773b",
   "metadata": {},
   "source": [
    "Phase 1 - Data Preprocessing \n",
    "1. Create Combination of Data for HeartRate to detect activity imbalances\n",
    "2. Create ManualInput Dataset \n",
    "3. Clean all datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eb7ac",
   "metadata": {},
   "source": [
    "Step 1: Create the HeartRate and METS merged dataset, filter null values and reduce the memory usage first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a32175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDataset(df):\n",
    "    print('Filter Dataset')\n",
    "\n",
    "    # view details of the dataset\n",
    "    print(df.head())\n",
    "    print('Column Names:',df.dtypes)\n",
    "\n",
    "    # print null values in the dataframe\n",
    "    print('The sum of null values are:', df.isnull().sum())\n",
    "    \n",
    "    \n",
    "    # drop rows with null values\n",
    "    print('Count of cells BEFORE dropping null:', df.size,'\\n')\n",
    "    df = df.dropna() \n",
    "    print('Count of cells AFTER dropping null:', df.size, '\\n')\n",
    "    print('------------------------------------------------------------------')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed48b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDateTime(df):\n",
    "    # datetime split\n",
    "    split_datetime = df['timestamp'].str.split(' ', expand=True)\n",
    "\n",
    "    # Assign the date and time components to new columns\n",
    "    df['date'] = split_datetime[0]\n",
    "    df['time'] = split_datetime[1]\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['time'] = df['time'].astype(str)\n",
    "\n",
    "    df.drop(columns=['timestamp'], inplace=True)\n",
    "    df[:3]\n",
    "    print('------------------------------------------------------------------')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "227be2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceMemoryUsage(df, verbose=True):\n",
    "    print('Reduce Memory')\n",
    "    \n",
    "    numerics = {\n",
    "        np.int8: (np.iinfo(np.int8).min,np.iinfo(np.int8).max),\n",
    "        np.int16: (np.iinfo(np.int16).min,np.iinfo(np.int16).max), \n",
    "        np.int32: (np.iinfo(np.int32).min,np.iinfo(np.int32).max), \n",
    "        np.int64: (np.iinfo(np.int64).min,np.iinfo(np.int64).max), \n",
    "        np.float16: (np.finfo(np.float16).min,np.finfo(np.float16).max), \n",
    "        np.float32: (np.finfo(np.float32).min,np.finfo(np.float32).max), \n",
    "        np.float64: (np.finfo(np.float64).min,np.finfo(np.float64).max)\n",
    "        }\n",
    "    types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_memory_usage = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    print('Starting memory usage is {:5.5f}'.format(start_memory_usage))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in types: \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            for n_key, n_value in numerics.items(): \n",
    "                if c_min > n_value[0] and c_max < n_value[1]:\n",
    "                    df[col] = df[col].astype(n_key)\n",
    "                    break\n",
    "    \n",
    "    end_memory_usage = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    if verbose: \n",
    "        print('Memory usage decreased to {:5.5f} Mb ({:.5f}% reduction)'.format(end_memory_usage, 100 * (start_memory_usage - end_memory_usage) / start_memory_usage))\n",
    "    print('------------------------------------------------------------------')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe313b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutliers(group, target='bpm'):\n",
    "   z_scores = stats.zscore(group[target])\n",
    "   threshold = 3\n",
    "   outlier_indices = group.index[abs(z_scores) > threshold]\n",
    "   return group.drop(outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9787ef6-d784-4e19-ba4e-402b198477c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHRMetsDataset():\n",
    "    ## Merge cleaned dataframes\n",
    "    \n",
    "    # read the heartrate and mets dataframe\n",
    "    df_mets = pd.read_csv('data_raw/RAW-Fitabase Data 4.12.16-5.12.16/minuteMETsNarrow_merged.csv')\n",
    "    df_heartrate = pd.read_csv('data_raw/RAW-Fitabase Data 4.12.16-5.12.16/heartrate_seconds_merged.csv')\n",
    "    df_intensities = pd.read_csv('data_raw/RAW-Fitabase Data 4.12.16-5.12.16/minuteIntensitiesNarrow_merged.csv')\n",
    "    \n",
    "    # update columns names to be the same\n",
    "    df_mets.columns = ['id', 'timestamp', 'mets']\n",
    "    df_heartrate.columns = ['id', 'timestamp', 'bpm']\n",
    "    df_intensities.columns = ['id','timestamp', 'intensity_level']\n",
    "        \n",
    "    # clean dataframes and reduce memory usage\n",
    "    df_mets = filterDataset(df_mets)\n",
    "    df_mets = reduceMemoryUsage(df_mets)\n",
    "    \n",
    "    df_heartrate = filterDataset(df_heartrate)\n",
    "    df_heartrate = reduceMemoryUsage(df_heartrate)\n",
    "   \n",
    "    df_intensities = filterDataset(df_intensities) \n",
    "    df_intensities = reduceMemoryUsage(df_intensities) \n",
    "    \n",
    "    # merge dataframes using the column names and with an inner join\n",
    "    df_merged_outer = pd.merge(df_mets, df_heartrate, on=['id', 'timestamp'], how='outer')\n",
    "    df_merged_inner = pd.merge(df_mets, df_heartrate, on=['id', 'timestamp'], how='inner')\n",
    "    df_merged_right = pd.merge(df_mets, df_heartrate, on=['id', 'timestamp'], how='right')\n",
    "    \n",
    "    df_merged_outer = pd.merge(df_intensities, df_merged_outer, on=['id', 'timestamp'], how='outer')\n",
    "    df_merged_inner = pd.merge(df_intensities, df_merged_inner, on=['id', 'timestamp'], how='inner')\n",
    "    df_merged_right = pd.merge(df_intensities, df_merged_right, on=['id', 'timestamp'], how='right')\n",
    "    \n",
    "    # remove outliers \n",
    "    g1 = df_merged_outer.groupby('intensity_level')\n",
    "    g2 = df_merged_inner.groupby('intensity_level')\n",
    "    g3 = df_merged_right.groupby('intensity_level')\n",
    "    \n",
    "    df_merged_outer = g1.apply(removeOutliers)\n",
    "    df_merged_inner = g2.apply(removeOutliers) \n",
    "    df_merged_right = g3.apply(removeOutliers)\n",
    "   \n",
    "    df_merged_inner.to_csv('data_interim/heartrate_mets_intensities_merged_outer.csv', index=False)\n",
    "    df_merged_inner.to_csv('data_interim/heartrate_mets_intensities_merged_inner.csv', index=False)\n",
    "    df_merged_right.to_csv('data_interim/heartrate_mets_intensities_merged_right.csv', index=False)\n",
    "    \n",
    "    # create heartrate dataset of cleaned results\n",
    "    df_heartrate.to_csv('data_interim/heartrate_fiveseconds_intervals.csv', index=False)\n",
    "   \n",
    "    print('Heartrate, METS, and minuteIntensity dataframes are merged')\n",
    "    df_merged_inner[:3]\n",
    "    df_merged_right[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d87cf5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter Dataset\n",
      "           id              timestamp  mets\n",
      "0  1503960366  4/12/2016 12:00:00 AM    10\n",
      "1  1503960366  4/12/2016 12:01:00 AM    10\n",
      "2  1503960366  4/12/2016 12:02:00 AM    10\n",
      "3  1503960366  4/12/2016 12:03:00 AM    10\n",
      "4  1503960366  4/12/2016 12:04:00 AM    10\n",
      "Column Names: id            int64\n",
      "timestamp    object\n",
      "mets          int64\n",
      "dtype: object\n",
      "The sum of null values are: id           0\n",
      "timestamp    0\n",
      "mets         0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 3976740 \n",
      "\n",
      "Count of cells AFTER dropping null: 3976740 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 117.53323\n",
      "Memory usage decreased to 109.94820 Mb (6.45352% reduction)\n",
      "------------------------------------------------------------------\n",
      "Filter Dataset\n",
      "           id             timestamp  bpm\n",
      "0  2022484408  4/12/2016 7:21:00 AM   97\n",
      "1  2022484408  4/12/2016 7:21:05 AM  102\n",
      "2  2022484408  4/12/2016 7:21:10 AM  105\n",
      "3  2022484408  4/12/2016 7:21:20 AM  103\n",
      "4  2022484408  4/12/2016 7:21:25 AM  101\n",
      "Column Names: id            int64\n",
      "timestamp    object\n",
      "bpm           int64\n",
      "dtype: object\n",
      "The sum of null values are: id           0\n",
      "timestamp    0\n",
      "bpm          0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 7450974 \n",
      "\n",
      "Count of cells AFTER dropping null: 7450974 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 220.17280\n",
      "Memory usage decreased to 205.96120 Mb (6.45475% reduction)\n",
      "------------------------------------------------------------------\n",
      "Filter Dataset\n",
      "           id              timestamp  intensity_level\n",
      "0  1503960366  4/12/2016 12:00:00 AM                0\n",
      "1  1503960366  4/12/2016 12:01:00 AM                0\n",
      "2  1503960366  4/12/2016 12:02:00 AM                0\n",
      "3  1503960366  4/12/2016 12:03:00 AM                0\n",
      "4  1503960366  4/12/2016 12:04:00 AM                0\n",
      "Column Names: id                  int64\n",
      "timestamp          object\n",
      "intensity_level     int64\n",
      "dtype: object\n",
      "The sum of null values are: id                 0\n",
      "timestamp          0\n",
      "intensity_level    0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 3976740 \n",
      "\n",
      "Count of cells AFTER dropping null: 3976740 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 117.53323\n",
      "Memory usage decreased to 108.68403 Mb (7.52911% reduction)\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\AppData\\Local\\Temp\\ipykernel_28164\\1801880065.py:38: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_merged_outer = g1.apply(removeOutliers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heartrate, METS, and minuteIntensity dataframes are merged\n"
     ]
    }
   ],
   "source": [
    "## create the heartrate_mets_merged.csv file with processed data\n",
    "createHRMetsDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d7c95e",
   "metadata": {},
   "source": [
    "Normalize the HRMets Dataset - \n",
    "1. View the Minimum METS & Heart rate to determine activity patterns in comparison with the intensities dataset\n",
    "2. look into the min and max values to see the best way to view these outliers and make sure the conditions are accurate --> add activity level column to filter when tracking anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36738be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def analyzeHRMetsDataset():\n",
    "    interim_data_path = f'{os.getcwd()}/data_interim/'\n",
    "    df_right = pd.read_csv(interim_data_path+'heartrate_mets_merged_right.csv')\n",
    "    \n",
    "    \n",
    "\n",
    "    # Define the min and max values for BPM and METs\n",
    "    bpm_min = df_right['bpm'].min()\n",
    "    bpm_max = df_right['bpm'].max()\n",
    "    mets_min = df_right['mets'].min()\n",
    "    mets_max = df_right['mets'].max()\n",
    "\n",
    "    print('BPM MIN:',bpm_min)\n",
    "    print('BPM MAX:',bpm_max)\n",
    "    \n",
    "    print('METS MIN:',mets_min)\n",
    "    print('METS MAX:',mets_max)\n",
    "\n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    # Fit the scaler on the original data range\n",
    "    scaler.fit([[bpm_min, mets_min], [bpm_max, mets_max]])\n",
    "\n",
    "    # Transform the original data to the scaled range\n",
    "    normalized_bpm_min, normalized_mets_min = scaler.transform([[bpm_min, mets_min]])[0]\n",
    "    normalized_bpm_max, normalized_mets_max = scaler.transform([[bpm_max, mets_max]])[0]\n",
    "\n",
    "    print(\"Normalized BPM MIN:\", normalized_bpm_min)\n",
    "    print(\"Normalized BPM MAX:\", normalized_bpm_max)\n",
    "    print(\"Normalized METS MIN:\", normalized_mets_min)\n",
    "    print(\"Normalized METS MAX:\", normalized_mets_max)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14ec08d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPM MIN: 36\n",
      "BPM MAX: 203\n",
      "METS MIN: 10.0\n",
      "METS MAX: 144.0\n",
      "Normalized BPM MIN: 0.0\n",
      "Normalized BPM MAX: 1.0000000000000002\n",
      "Normalized METS MIN: 0.0\n",
      "Normalized METS MAX: 1.0\n"
     ]
    }
   ],
   "source": [
    "analyzeHRMetsDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc439b1f",
   "metadata": {},
   "source": [
    "Step 2: Sleep & Actvity Tracking Dataset Cleaning (Select the most important attributes and convert each dataset into interim ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a626bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processContextualDatasets():\n",
    "    raw_data_path = f'{os.getcwd()}/data_raw/RAW-Fitabase Data 4.12.16-5.12.16/'\n",
    "    \n",
    "    ## Cleaning SleepDay Dataset\n",
    "    df_sleepDay = pd.read_csv(raw_data_path+'sleepDay_merged.csv')\n",
    "    df_sleepDay.columns = ['id', 'timestamp', 'total_sleep_records', 'total_minutes_asleep', 'total_time_inbed']\n",
    "    df_sleepDay.drop(columns=['total_time_inbed'], inplace=True)\n",
    "    \n",
    "    df_sleepDay = filterDataset(df_sleepDay)\n",
    "    df_sleepDay = reduceMemoryUsage(df_sleepDay)\n",
    "    df_sleepDay = parseDateTime(df_sleepDay)\n",
    "    df_sleepDay.drop(columns=['time'], inplace=True)\n",
    "    \n",
    "    # Write Dataset to csv \n",
    "    interim_data_path = f'{os.getcwd()}/data_interim/'\n",
    "    df_sleepDay.to_csv(f'{interim_data_path}daily_sleep.csv', index=False)\n",
    "    \n",
    "    ## Cleaning Activity Dataset\n",
    "    df_dailyActivity = pd.read_csv(f'{raw_data_path}dailyActivity_merged.csv')\n",
    "    df_dailyActivity = df_dailyActivity.iloc[:,[0,1,2,3,10,11,12,13,14]]\n",
    "    df_dailyActivity.columns = ['id', 'date', 'total_steps', 'total_distance_miles', 'very_active_minutes', 'fairly_active_minutes', 'lightly_active_minutes', 'sedentary_minutes', 'calories']\n",
    "\n",
    "    df_dailyActivity = filterDataset(df_dailyActivity)\n",
    "    df_dailyActivity = reduceMemoryUsage(df_dailyActivity)\n",
    "    \n",
    "    # merge sleep day and activity day\n",
    "    df_merged = pd.merge(df_sleepDay, df_dailyActivity, on=['id', 'date'], how='inner')\n",
    "    df_merged.to_csv(f'{interim_data_path}daily_sleep_activity.csv', index=False)\n",
    "    \n",
    "    print('Daily Activity and Sleep is Merged')\n",
    "    df_merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6d181f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter Dataset\n",
      "           id              timestamp  total_sleep_records  \\\n",
      "0  1503960366  4/12/2016 12:00:00 AM                    1   \n",
      "1  1503960366  4/13/2016 12:00:00 AM                    2   \n",
      "2  1503960366  4/15/2016 12:00:00 AM                    1   \n",
      "3  1503960366  4/16/2016 12:00:00 AM                    2   \n",
      "4  1503960366  4/17/2016 12:00:00 AM                    1   \n",
      "\n",
      "   total_minutes_asleep  \n",
      "0                   327  \n",
      "1                   384  \n",
      "2                   412  \n",
      "3                   340  \n",
      "4                   700  \n",
      "Column Names: id                       int64\n",
      "timestamp               object\n",
      "total_sleep_records      int64\n",
      "total_minutes_asleep     int64\n",
      "dtype: object\n",
      "The sum of null values are: id                      0\n",
      "timestamp               0\n",
      "total_sleep_records     0\n",
      "total_minutes_asleep    0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 1652 \n",
      "\n",
      "Count of cells AFTER dropping null: 1652 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 0.04018\n",
      "Memory usage decreased to 0.03506 Mb (12.74177% reduction)\n",
      "------------------------------------------------------------------\n",
      "------------------------------------------------------------------\n",
      "Filter Dataset\n",
      "           id       date  total_steps  total_distance_miles  \\\n",
      "0  1503960366  4/12/2016        13162                  8.50   \n",
      "1  1503960366  4/13/2016        10735                  6.97   \n",
      "2  1503960366  4/14/2016        10460                  6.74   \n",
      "3  1503960366  4/15/2016         9762                  6.28   \n",
      "4  1503960366  4/16/2016        12669                  8.16   \n",
      "\n",
      "   very_active_minutes  fairly_active_minutes  lightly_active_minutes  \\\n",
      "0                   25                     13                     328   \n",
      "1                   21                     19                     217   \n",
      "2                   30                     11                     181   \n",
      "3                   29                     34                     209   \n",
      "4                   36                     10                     221   \n",
      "\n",
      "   sedentary_minutes  calories  \n",
      "0                728      1985  \n",
      "1                776      1797  \n",
      "2               1218      1776  \n",
      "3                726      1745  \n",
      "4                773      1863  \n",
      "Column Names: id                          int64\n",
      "date                       object\n",
      "total_steps                 int64\n",
      "total_distance_miles      float64\n",
      "very_active_minutes         int64\n",
      "fairly_active_minutes       int64\n",
      "lightly_active_minutes      int64\n",
      "sedentary_minutes           int64\n",
      "calories                    int64\n",
      "dtype: object\n",
      "The sum of null values are: id                        0\n",
      "date                      0\n",
      "total_steps               0\n",
      "total_distance_miles      0\n",
      "very_active_minutes       0\n",
      "fairly_active_minutes     0\n",
      "lightly_active_minutes    0\n",
      "sedentary_minutes         0\n",
      "calories                  0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 8460 \n",
      "\n",
      "Count of cells AFTER dropping null: 8460 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 0.11642\n",
      "Memory usage decreased to 0.07966 Mb (31.57205% reduction)\n",
      "------------------------------------------------------------------\n",
      "Daily Activity and Sleep is Merged\n"
     ]
    }
   ],
   "source": [
    "processContextualDatasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaba711",
   "metadata": {},
   "source": [
    "Step 3: Create ManualInput Dataset\n",
    "Columns = id, date, time, blood pressure, glucose_morning, glucose_evening weight, calorie consumption, symptoms, mental health - Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0feaf42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateManualInputMetadata():\n",
    "    # read the weightLogInfo_merged.csv file and keep the id, date, weight kg/pounds, fat, bmi\n",
    "    raw_data_path = f'{os.getcwd()}/data_raw/RAW-Fitabase Data 4.12.16-5.12.16/'\n",
    "    df_weight_log_info = pd.read_csv(raw_data_path+'weightLogInfo_merged.csv')\n",
    "    df_weight_log_info.drop(columns=['IsManualReport', 'LogId','Fat'], inplace=True)\n",
    "\n",
    "    # clean dataset \n",
    "    df_weight_log_info.rename(columns={'Date':'timestamp'},inplace=True)\n",
    "    print('Head')\n",
    "    df_weight_log_info.head()\n",
    "    df_weight_log_info = filterDataset(df_weight_log_info)\n",
    "    df_weight_log_info = reduceMemoryUsage(df_weight_log_info)\n",
    "    df_weight_log_info = parseDateTime(df_weight_log_info)\n",
    "    df_weight_log_info.columns=['id','weight_kg', 'weight_pounds', 'bmi','date','time']\n",
    "    \n",
    "    ## create final intended synthetic dataset\n",
    "    df_manual_input = pd.DataFrame(columns = ['id', 'date', 'time', 'weight_kg', 'weight_pounds', 'fat', 'bmi', 'calorie_consumption','doctor_visit', 'symptom_code', 'blood_pressure', 'glucose_morning','glucose_evening','mental_health_code'])\n",
    "    \n",
    "    # populate manual_input with ids and date data\n",
    "    start_date = '2016-04-12'\n",
    "    end_date = '2016-05-12'\n",
    "    num_days = (datetime(2016, 5, 12) - datetime(2016, 4, 12)).days \n",
    "    num_days += 1\n",
    "\n",
    "    log_info = list(df_weight_log_info['id'].unique())\n",
    "    print(log_info)\n",
    "    df_manual_input = pd.DataFrame({\n",
    "        'id': pd.Series(log_info).repeat(num_days),\n",
    "        'date': pd.date_range(start=start_date, end=end_date, freq='D').repeat(len(log_info)),\n",
    "        'fat': [0] * (num_days * len(log_info)), \n",
    "        'calorie_consumption': [0] * (num_days * len(log_info)),\n",
    "        'doctor_visit': [False] * (num_days * len(log_info)), \n",
    "        'symptom_code': [0] * (num_days * len(log_info)), \n",
    "        'blood_pressure': [0] * (num_days * len(log_info)), \n",
    "        'glucose_morning': [0] * (num_days * len(log_info)),\n",
    "        'glucose_evening': [0] * (num_days * len(log_info)),\n",
    "        'mental_health_code': [0] * (num_days * len(log_info))\n",
    "    })\n",
    "    \n",
    "    # aligning datatypes\n",
    "    df_manual_input['date'] = pd.to_datetime(df_manual_input['date'])\n",
    "    df_weight_log_info['date'] = pd.to_datetime(df_weight_log_info['date'])\n",
    "    \n",
    "    # Place cleaned weight_log_info into the manual_input df\n",
    "    df_manual_input = pd.merge(df_weight_log_info,df_manual_input,on=['id','date'],how='outer')\n",
    "    interim_data_path = f'{os.getcwd()}/data_interim/'\n",
    "    df_manual_input.to_csv(f'{interim_data_path}manual_input.csv', index=False)\n",
    "    print(df_manual_input.head())\n",
    "    \n",
    "    df_manual_input.info()\n",
    "    ## Generate Synthetic Dataset\n",
    "    # define constraints for dataset\n",
    "    \n",
    "    metadata = SingleTableMetadata()\n",
    "    metadata.detect_from_dataframe(df_manual_input) \n",
    "    \n",
    "    # # dictionary corresponding to the metadata\n",
    "    python_dict = metadata.to_dict()\n",
    "    metadata.visualize(\n",
    "        show_table_details='full',\n",
    "        output_filepath='visualizations/manualInput.png'    \n",
    "    )\n",
    "    metadata.validate()\n",
    "    metadata.validate_data(data=df_manual_input)\n",
    "    \n",
    "    # update columnn types\n",
    "    metadata.update_column(\n",
    "        column_name='id',\n",
    "        sdtype='id',\n",
    "    )   \n",
    "    metadata.update_column(\n",
    "        column_name='doctor_visit',\n",
    "        sdtype='boolean',\n",
    "    )   \n",
    "    \n",
    "    # update the datatype visualization\n",
    "    metadata.visualize(\n",
    "        show_table_details='full',\n",
    "        output_filepath='visualizations/manualInput.png'    \n",
    "    )\n",
    "    \n",
    "    # save as json for future use \n",
    "    metadata.save_to_json(filepath='metadata/metadata_manual_input.json')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6bebb20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head\n",
      "Filter Dataset\n",
      "           Id              timestamp    WeightKg  WeightPounds        BMI\n",
      "0  1503960366   5/2/2016 11:59:59 PM   52.599998    115.963147  22.650000\n",
      "1  1503960366   5/3/2016 11:59:59 PM   52.599998    115.963147  22.650000\n",
      "2  1927972279   4/13/2016 1:08:52 AM  133.500000    294.317120  47.540001\n",
      "3  2873212765  4/21/2016 11:59:59 PM   56.700001    125.002104  21.450001\n",
      "4  2873212765  5/12/2016 11:59:59 PM   57.299999    126.324875  21.690001\n",
      "Column Names: Id                int64\n",
      "timestamp        object\n",
      "WeightKg        float64\n",
      "WeightPounds    float64\n",
      "BMI             float64\n",
      "dtype: object\n",
      "The sum of null values are: Id              0\n",
      "timestamp       0\n",
      "WeightKg        0\n",
      "WeightPounds    0\n",
      "BMI             0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 335 \n",
      "\n",
      "Count of cells AFTER dropping null: 335 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 0.00711\n",
      "Memory usage decreased to 0.00589 Mb (17.08496% reduction)\n",
      "------------------------------------------------------------------\n",
      "------------------------------------------------------------------\n",
      "[1503960366, 1927972279, 2873212765, 4319703577, 4558609924, 5577150313, 6962181067, 8877689391]\n",
      "           id  weight_kg  weight_pounds   bmi       date      time  fat  \\\n",
      "0  1503960366       52.0          115.0  22.0 2016-05-02  11:59:59  NaN   \n",
      "1  1503960366       52.0          115.0  22.0 2016-05-03  11:59:59  NaN   \n",
      "2  1927972279      133.0          294.0  47.0 2016-04-13   1:08:52  NaN   \n",
      "3  2873212765       56.0          125.0  21.0 2016-04-21  11:59:59  0.0   \n",
      "4  2873212765       56.0          125.0  21.0 2016-04-21  11:59:59  0.0   \n",
      "\n",
      "   calorie_consumption doctor_visit  symptom_code  blood_pressure  \\\n",
      "0                  NaN          NaN           NaN             NaN   \n",
      "1                  NaN          NaN           NaN             NaN   \n",
      "2                  NaN          NaN           NaN             NaN   \n",
      "3                  0.0        False           0.0             0.0   \n",
      "4                  0.0        False           0.0             0.0   \n",
      "\n",
      "   glucose_morning  glucose_evening  mental_health_code  \n",
      "0              NaN              NaN                 NaN  \n",
      "1              NaN              NaN                 NaN  \n",
      "2              NaN              NaN                 NaN  \n",
      "3              0.0              0.0                 0.0  \n",
      "4              0.0              0.0                 0.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 305 entries, 0 to 304\n",
      "Data columns (total 14 columns):\n",
      " #   Column               Non-Null Count  Dtype         \n",
      "---  ------               --------------  -----         \n",
      " 0   id                   305 non-null    int64         \n",
      " 1   weight_kg            122 non-null    float64       \n",
      " 2   weight_pounds        122 non-null    float64       \n",
      " 3   bmi                  122 non-null    float64       \n",
      " 4   date                 305 non-null    datetime64[ns]\n",
      " 5   time                 122 non-null    object        \n",
      " 6   fat                  248 non-null    float64       \n",
      " 7   calorie_consumption  248 non-null    float64       \n",
      " 8   doctor_visit         248 non-null    object        \n",
      " 9   symptom_code         248 non-null    float64       \n",
      " 10  blood_pressure       248 non-null    float64       \n",
      " 11  glucose_morning      248 non-null    float64       \n",
      " 12  glucose_evening      248 non-null    float64       \n",
      " 13  mental_health_code   248 non-null    float64       \n",
      "dtypes: datetime64[ns](1), float64(10), int64(1), object(2)\n",
      "memory usage: 35.7+ KB\n"
     ]
    }
   ],
   "source": [
    "generateManualInputMetadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc617b51",
   "metadata": {},
   "source": [
    "Step 4: Complete the Manual Input dataframe generation - Modeling Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9eeaa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillManualInputMetadata():\n",
    "    metadata = SingleTableMetadata.load_from_json(filepath='metadata/metadata_manual_input.json')\n",
    "    \n",
    "    interim_data_path = f'{os.getcwd()}/data_interim/'\n",
    "    df_manual_input = pd.read_csv(interim_data_path+'manual_input.csv')\n",
    "    # anonymize digital data for sharing purposes\n",
    "    # anonymized_metadata = metadata.anonymize()\n",
    "    \n",
    "    # create and train the synthesizer\n",
    "    s = GaussianCopulaSynthesizer(\n",
    "        metadata=metadata,\n",
    "        enforce_min_max_values=True, \n",
    "        enforce_rounding=False,\n",
    "        numerical_distributions={\n",
    "        'id': 'norm',\n",
    "        'date': 'norm',\n",
    "        'time': 'norm',\n",
    "        'weight_kg': 'uniform',\n",
    "        'weight_pounds': 'uniform',\n",
    "        'fat': 'norm',\n",
    "        'bmi': 'norm',\n",
    "        'calorie_consumption': 'norm',\n",
    "        'doctor_visit': 'norm',\n",
    "        'symptom_code': 'norm',\n",
    "        'blood_pressure': 'norm',\n",
    "        'glucose_morning': 'norm',\n",
    "        'glucose_evening': 'norm',\n",
    "        'mental_health_code': 'norm'\n",
    "        },\n",
    "        default_distribution='norm'\n",
    "        )\n",
    "    s.fit(df_manual_input)\n",
    "    s_data = s.sample(num_rows=10) # test with 10 rows\n",
    "    \n",
    "    # get the metadata with s.get_metadata()\n",
    "    # view the shape estimates based on the distribution\n",
    "    s.get_learned_distributions()\n",
    "    # # set constraints\n",
    "    # constraints = [\n",
    "    # {\n",
    "    # 'constraint_class': 'ScalarInequality',\n",
    "    # 'constraint_parameters': {\n",
    "    #     'column_name': 'date',\n",
    "    #     'relation': '>=',\n",
    "    #     'value': '2016-04-12'\n",
    "    #     }   \n",
    "    # },\n",
    "    # {\n",
    "    # 'constraint_class': 'ScalarInequality',\n",
    "    # 'constraint_parameters': {\n",
    "    #     'column_name': 'date',\n",
    "    #     'relation': '<=',\n",
    "    #     'value': '2016-05-12'\n",
    "    #     }   \n",
    "    # },\n",
    "    # {\n",
    "    # 'constraint_class': 'FixedIncrements',\n",
    "    # 'constraint_parameters': {\n",
    "    #     'column_name': 'date',  # Adjust the column name accordingly\n",
    "    #     'increment': '1 day'  # Adjust the increment as needed\n",
    "    #     }  \n",
    "    # },\n",
    "    # {\n",
    "    # 'constraint_class': 'ScalarInequality',\n",
    "    # 'constraint_parameters': {\n",
    "    #     'column_name': 'weight_pounds',  # Adjust the column name accordingly\n",
    "    #     'relation': 'between',  # Define the relation as 'between'\n",
    "    #     'low_value': -15,  # Set the lower bound as -15 pounds or kilograms\n",
    "    #     'high_value': 15  # Set the upper bound as 15 pounds or kilograms\n",
    "    #     }\n",
    "    # },\n",
    "    #     {\n",
    "    # 'constraint_class': 'ScalarInequality',\n",
    "    # 'constraint_parameters': {\n",
    "    #     'column_name': 'weight_kg',  # Adjust the column name accordingly\n",
    "    #     'relation': 'between',  # Define the relation as 'between'\n",
    "    #     'low_value': -6.803885,  # Set the lower bound as -15 pounds or kilograms\n",
    "    #     'high_value': -6.803885  # Set the upper bound as 15 pounds or kilograms\n",
    "    #     }\n",
    "    # },\n",
    "    # {\n",
    "    # 'constraint_class': 'ScalarRange',\n",
    "    # 'constraint_parameters': {\n",
    "    #     'column_name': 'weight_kg',\n",
    "    #     'low_value': 54.4311,  # 120 lbs converted to kg\n",
    "    #     'high_value': 136.0777  # 300 lbs converted to kg\n",
    "    # }\n",
    "    # },\n",
    "    # {\n",
    "    #     'constraint_class': 'ScalarRange',\n",
    "    #     'constraint_parameters': {\n",
    "    #         'column_name': 'weight_pounds',\n",
    "    #         'low_value': 120,\n",
    "    #         'high_value': 300\n",
    "    #     }\n",
    "    # },\n",
    "    # {\n",
    "    #     'constraint_class': 'ScalarRange',\n",
    "    #     'constraint_parameters': {\n",
    "    #         'column_name': 'blood_pressure',\n",
    "    #         'low_value': 90,  # Typical diastolic pressure\n",
    "    #         'high_value': 120  # Typical systolic pressure\n",
    "    #     }\n",
    "    # }\n",
    "\n",
    "    # ]\n",
    "    # s.add_constraints(constraints=constraints)\n",
    "    s.auto_assign_transformers(df_manual_input)\n",
    "    print(s.get_transformers())\n",
    "    \n",
    "    # preprocess data using the transformation\n",
    "    processed_data = s.preprocess(df_manual_input)\n",
    "    s.fit_processed_data(processed_data=processed_data)\n",
    "    s.save(filepath='visualizations/manual_input.pkl')\n",
    "    \n",
    "    # save 300 samples to csv\n",
    "    synthetic_data = s.sample(num_rows = 300)\n",
    "    synthetic_data.to_csv('data_interim/synthetic_data.csv', index=False)\n",
    "    \n",
    "    \n",
    "    # load the synthesizer\n",
    "    # s = GaussianCopulaSynthesizer.load(filepath='synthesizer_manual_input.pkl')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c35c1f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': AnonymizedFaker(function_name='bothify', function_kwargs={'text': '#####'}), 'weight_kg': FloatFormatter(enforce_min_max_values=True), 'weight_pounds': FloatFormatter(enforce_min_max_values=True), 'bmi': UniformEncoder(), 'date': UnixTimestampEncoder(datetime_format='%Y-%m-%d', enforce_min_max_values=True), 'time': UniformEncoder(), 'fat': UniformEncoder(), 'calorie_consumption': UniformEncoder(), 'doctor_visit': UniformEncoder(), 'symptom_code': UniformEncoder(), 'blood_pressure': UniformEncoder(), 'glucose_morning': UniformEncoder(), 'glucose_evening': UniformEncoder(), 'mental_health_code': UniformEncoder()}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\AppData\\Roaming\\Python\\Python311\\site-packages\\sdv\\single_table\\base.py:354: UserWarning: This model has already been fitted. To use the new preprocessed data, please refit the model using 'fit' or 'fit_processed_data'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "fillManualInputMetadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bf6fa",
   "metadata": {},
   "source": [
    "Step 5. Generate ManualInputDataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e96bfe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateManualInput():\n",
    "    ids = [150390366, 1927972279, 2873212765, 4319703577, 4558609924, 5577150313, 6962181067, 8877689391]\n",
    "    start_date = datetime(2016, 4, 12)\n",
    "    end_date = datetime(2016, 5, 12)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    data = []\n",
    "    for _id in ids:\n",
    "        initial_kg = random.uniform(50, 120)\n",
    "        initial_bmi = random.uniform(18.5, 30)\n",
    "        initial_fat = random.uniform(15, 35)\n",
    "        direction_two = 0\n",
    "        for date in date_range:\n",
    "            weight_kg = initial_kg + direction_two\n",
    "            weight_pounds = initial_kg * 2.20462\n",
    "            bmi = initial_bmi + direction_two\n",
    "            timestamp = date\n",
    "            fat = initial_fat + direction_two\n",
    "            calorie_consumption = random.randint(1000, 3000)\n",
    "            doctor_visit = random.choice([True, False])\n",
    "            symptom_code = random.choice(['Nausea and Vomiting','Fatigue','None','Headaches','Back Pain','Swelling in Extremities','Heartburn','Constipation','None','Frequent Urination','Braxton Hicks Contractions','Round Ligament Pain', 'None'])\n",
    "            blood_pressure = f\"{random.randint(90, 120)}/{random.randint(60, 80)}\"\n",
    "            glucose_morning = random.randint(70, 110)\n",
    "            glucose_evening = random.randint(70, 110)\n",
    "            mental_health_code = random.choice( ['Anxiety','Depression','None','Stress','Mood Swings','Insomnia','Postpartum Depression','Adjustment Disorder','None','Pregnancy-related OCD','Body Image Issues','Relationship Strain', 'Other', 'None'])\n",
    "\n",
    "            data.append([_id, weight_kg, weight_pounds, bmi, timestamp, fat, calorie_consumption, \n",
    "                         doctor_visit, symptom_code, blood_pressure, glucose_morning, \n",
    "                         glucose_evening, mental_health_code])\n",
    "            \n",
    "            direction_two = random.choice([-2, -1, 0, 1, 2])\n",
    "\n",
    "    df_manual_input = pd.DataFrame(data, columns=['id', 'weight_kg', 'weight_pounds', 'bmi', 'timestamp', \n",
    "                                     'fat', 'calorie_consumption', 'doctor_visit', 'symptom_code', \n",
    "                                     'blood_pressure', 'glucose_morning', 'glucose_evening', \n",
    "                                     'mental_health_code'])\n",
    "\n",
    "    \n",
    "    print(df_manual_input.head())\n",
    "    df_manual_input.to_csv('data_interim/manual_input.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "068ce137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id   weight_kg  weight_pounds       bmi  timestamp        fat  \\\n",
      "0  150390366  110.519697     243.653935  24.81896 2016-04-12  18.969158   \n",
      "1  150390366  108.519697     243.653935  22.81896 2016-04-13  16.969158   \n",
      "2  150390366  108.519697     243.653935  22.81896 2016-04-14  16.969158   \n",
      "3  150390366  109.519697     243.653935  23.81896 2016-04-15  17.969158   \n",
      "4  150390366  110.519697     243.653935  24.81896 2016-04-16  18.969158   \n",
      "\n",
      "   calorie_consumption  doctor_visit                symptom_code  \\\n",
      "0                 2399         False                        None   \n",
      "1                 1890          True          Frequent Urination   \n",
      "2                 1250         False                        None   \n",
      "3                 2741         False  Braxton Hicks Contractions   \n",
      "4                 2659          True                        None   \n",
      "\n",
      "  blood_pressure  glucose_morning  glucose_evening     mental_health_code  \n",
      "0          92/78               78               93  Postpartum Depression  \n",
      "1         115/64              109              105            Mood Swings  \n",
      "2         115/61               75               91                 Stress  \n",
      "3         109/60              100               81    Adjustment Disorder  \n",
      "4         116/64               82              105             Depression  \n"
     ]
    }
   ],
   "source": [
    "generateManualInput()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
