{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4efc2fde-43f3-4659-88ec-524310a1cb5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn import preprocessing \n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f84a71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Predictive Model Folder: C:\\Users\\ashly\\OneDrive\\Documents\\Education Material\\ResearchProject\\MaternalHealthResearch\\predictive-model\n"
     ]
    }
   ],
   "source": [
    "# set working directory \n",
    "load_dotenv()\n",
    "os.chdir(os.getenv('DEFAULT_PATH'))\n",
    "\n",
    "print('In Predictive Model Folder:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f773b",
   "metadata": {},
   "source": [
    "Phase 1 - Data Preprocessing \n",
    "1. Create Combination of Data for HeartRate to detect activity imbalances\n",
    "2. Create ManualInput Dataset \n",
    "3. Clean all datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eb7ac",
   "metadata": {},
   "source": [
    "Step 1: Create the HeartRate and METS merged dataset, filter null values and reduce the memory usage first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a32175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDataset(df):\n",
    "    print('Filter Dataset')\n",
    "\n",
    "    # view details of the dataset\n",
    "    print(df.head())\n",
    "    print('Column Names:',df.dtypes)\n",
    "\n",
    "    # print null values in the dataframe\n",
    "    print('The sum of null values are:', df.isnull().sum())\n",
    "    \n",
    "    \n",
    "    # drop rows with null values\n",
    "    print('Count of cells BEFORE dropping null:', df.size,'\\n')\n",
    "    df = df.dropna() \n",
    "    print('Count of cells AFTER dropping null:', df.size, '\\n')\n",
    "    print('------------------------------------------------------------------')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed48b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDateTime(df):\n",
    "    # datetime split\n",
    "    split_datetime = df['timestamp'].str.split(' ', expand=True)\n",
    "\n",
    "    # Assign the date and time components to new columns\n",
    "    df['date'] = split_datetime[0]\n",
    "    df['time'] = split_datetime[1]\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['time'] = df['time'].astype(str)\n",
    "\n",
    "    df.drop(columns=['timestamp'], inplace=True)\n",
    "    df[:3]\n",
    "    print('------------------------------------------------------------------')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "227be2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceMemoryUsage(df, verbose=True):\n",
    "    print('Reduce Memory')\n",
    "    \n",
    "    numerics = {\n",
    "        np.int8: (np.iinfo(np.int8).min,np.iinfo(np.int8).max),\n",
    "        np.int16: (np.iinfo(np.int16).min,np.iinfo(np.int16).max), \n",
    "        np.int32: (np.iinfo(np.int32).min,np.iinfo(np.int32).max), \n",
    "        np.int64: (np.iinfo(np.int64).min,np.iinfo(np.int64).max), \n",
    "        np.float16: (np.finfo(np.float16).min,np.finfo(np.float16).max), \n",
    "        np.float32: (np.finfo(np.float32).min,np.finfo(np.float32).max), \n",
    "        np.float64: (np.finfo(np.float64).min,np.finfo(np.float64).max)\n",
    "        }\n",
    "    types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_memory_usage = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    print('Starting memory usage is {:5.5f}'.format(start_memory_usage))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in types: \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            for n_key, n_value in numerics.items(): \n",
    "                if c_min > n_value[0] and c_max < n_value[1]:\n",
    "                    df[col] = df[col].astype(n_key)\n",
    "                    break\n",
    "    \n",
    "    end_memory_usage = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    if verbose: \n",
    "        print('Memory usage decreased to {:5.5f} Mb ({:.5f}% reduction)'.format(end_memory_usage, 100 * (start_memory_usage - end_memory_usage) / start_memory_usage))\n",
    "    print('------------------------------------------------------------------')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebe313b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutliers(group, target='bpm'):\n",
    "   z_scores = stats.zscore(group[target])\n",
    "   threshold = 3\n",
    "   outlier_indices = group.index[abs(z_scores) > threshold]\n",
    "   return group.drop(outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36738be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeMinMax(df,column_name):\n",
    "    min_value = df[column_name].min()\n",
    "    max_value = df[column_name].max()\n",
    "    x = df[[column_name]].values\n",
    "    x = x.reshape(-1, 1)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df[column_name] = x_scaled\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9787ef6-d784-4e19-ba4e-402b198477c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHRMetsDataset():\n",
    "    ## Merge cleaned dataframes\n",
    "    \n",
    "    # read the heartrate and mets dataframe\n",
    "    df_mets = pd.read_csv('data_raw/RAW-Fitabase Data 4.12.16-5.12.16/minuteMETsNarrow_merged.csv')\n",
    "    df_heartrate = pd.read_csv('data_raw/RAW-Fitabase Data 4.12.16-5.12.16/heartrate_seconds_merged.csv')\n",
    "    df_intensities = pd.read_csv('data_raw/RAW-Fitabase Data 4.12.16-5.12.16/minuteIntensitiesNarrow_merged.csv')\n",
    "    \n",
    "    # update columns names to be the same\n",
    "    df_mets.columns = ['id', 'timestamp', 'mets']\n",
    "    df_heartrate.columns = ['id', 'timestamp', 'bpm']\n",
    "    df_intensities.columns = ['id','timestamp', 'intensity_level']\n",
    "        \n",
    "    # clean dataframes and reduce memory usage\n",
    "    df_mets = filterDataset(df_mets)\n",
    "    df_mets = reduceMemoryUsage(df_mets)\n",
    "    # df_mets = normalizeMinMax(df_mets, 'mets')\n",
    "    \n",
    "    df_heartrate = filterDataset(df_heartrate)\n",
    "    df_heartrate = reduceMemoryUsage(df_heartrate)\n",
    "   \n",
    "    df_intensities = filterDataset(df_intensities) \n",
    "    df_intensities = reduceMemoryUsage(df_intensities) \n",
    "    \n",
    "    # merge dataframes using the column names and with an inner join\n",
    "    df_merged_outer = pd.merge(df_mets, df_heartrate, on=['id', 'timestamp'], how='outer')\n",
    "    df_merged_inner = pd.merge(df_mets, df_heartrate, on=['id', 'timestamp'], how='inner')\n",
    "    df_merged_right = pd.merge(df_mets, df_heartrate, on=['id', 'timestamp'], how='right')\n",
    "    \n",
    "    df_merged_outer = pd.merge(df_intensities, df_merged_outer, on=['id', 'timestamp'], how='outer')\n",
    "    df_merged_inner = pd.merge(df_intensities, df_merged_inner, on=['id', 'timestamp'], how='inner')\n",
    "    df_merged_right = pd.merge(df_intensities, df_merged_right, on=['id', 'timestamp'], how='right')\n",
    "    \n",
    "    # remove outliers \n",
    "    g1 = df_merged_outer.groupby('intensity_level')\n",
    "    g2 = df_merged_inner.groupby('intensity_level')\n",
    "    g3 = df_merged_right.groupby('intensity_level')\n",
    "    \n",
    "    df_merged_outer = g1.apply(removeOutliers)\n",
    "    df_merged_inner = g2.apply(removeOutliers) \n",
    "    df_merged_right = g3.apply(removeOutliers)\n",
    "   \n",
    "    df_merged_inner.to_csv('data_interim/heartrate_mets_intensities_merged_outer.csv', index=False)\n",
    "    df_merged_inner.to_csv('data_interim/heartrate_mets_intensities_merged_inner.csv', index=False)\n",
    "    df_merged_right.to_csv('data_interim/heartrate_mets_intensities_merged_right.csv', index=False)\n",
    "    \n",
    "    # create heartrate dataset of cleaned results\n",
    "    df_heartrate.to_csv('data_interim/heartrate_fiveseconds_intervals.csv', index=False)\n",
    "   \n",
    "    print('Heartrate, METS, and minuteIntensity dataframes are merged')\n",
    "    df_merged_inner[:3]\n",
    "    df_merged_right[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d87cf5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter Dataset\n",
      "           id              timestamp  mets\n",
      "0  1503960366  4/12/2016 12:00:00 AM    10\n",
      "1  1503960366  4/12/2016 12:01:00 AM    10\n",
      "2  1503960366  4/12/2016 12:02:00 AM    10\n",
      "3  1503960366  4/12/2016 12:03:00 AM    10\n",
      "4  1503960366  4/12/2016 12:04:00 AM    10\n",
      "Column Names: id            int64\n",
      "timestamp    object\n",
      "mets          int64\n",
      "dtype: object\n",
      "The sum of null values are: id           0\n",
      "timestamp    0\n",
      "mets         0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 3976740 \n",
      "\n",
      "Count of cells AFTER dropping null: 3976740 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 117.53323\n",
      "Memory usage decreased to 109.94820 Mb (6.45352% reduction)\n",
      "------------------------------------------------------------------\n",
      "Filter Dataset\n",
      "           id             timestamp  bpm\n",
      "0  2022484408  4/12/2016 7:21:00 AM   97\n",
      "1  2022484408  4/12/2016 7:21:05 AM  102\n",
      "2  2022484408  4/12/2016 7:21:10 AM  105\n",
      "3  2022484408  4/12/2016 7:21:20 AM  103\n",
      "4  2022484408  4/12/2016 7:21:25 AM  101\n",
      "Column Names: id            int64\n",
      "timestamp    object\n",
      "bpm           int64\n",
      "dtype: object\n",
      "The sum of null values are: id           0\n",
      "timestamp    0\n",
      "bpm          0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 7450974 \n",
      "\n",
      "Count of cells AFTER dropping null: 7450974 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 220.17280\n",
      "Memory usage decreased to 205.96120 Mb (6.45475% reduction)\n",
      "------------------------------------------------------------------\n",
      "Filter Dataset\n",
      "           id              timestamp  intensity_level\n",
      "0  1503960366  4/12/2016 12:00:00 AM                0\n",
      "1  1503960366  4/12/2016 12:01:00 AM                0\n",
      "2  1503960366  4/12/2016 12:02:00 AM                0\n",
      "3  1503960366  4/12/2016 12:03:00 AM                0\n",
      "4  1503960366  4/12/2016 12:04:00 AM                0\n",
      "Column Names: id                  int64\n",
      "timestamp          object\n",
      "intensity_level     int64\n",
      "dtype: object\n",
      "The sum of null values are: id                 0\n",
      "timestamp          0\n",
      "intensity_level    0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 3976740 \n",
      "\n",
      "Count of cells AFTER dropping null: 3976740 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 117.53323\n",
      "Memory usage decreased to 108.68403 Mb (7.52911% reduction)\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\AppData\\Local\\Temp\\ipykernel_1204\\3070593689.py:39: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  df_merged_outer = g1.apply(removeOutliers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heartrate, METS, and minuteIntensity dataframes are merged\n"
     ]
    }
   ],
   "source": [
    "createHRMetsDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc439b1f",
   "metadata": {},
   "source": [
    "Step 2: Sleep & Actvity Tracking Dataset Cleaning (Select the most important attributes and convert each dataset into interim ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processContextualDatasets():\n",
    "    raw_data_path = f'{os.getcwd()}/data_raw/RAW-Fitabase Data 4.12.16-5.12.16/'\n",
    "    \n",
    "    ## Cleaning SleepDay Dataset\n",
    "    df_sleepDay = pd.read_csv(raw_data_path+'sleepDay_merged.csv')\n",
    "    df_sleepDay.columns = ['id', 'timestamp', 'total_sleep_records', 'total_minutes_asleep', 'total_time_inbed']\n",
    "    df_sleepDay.drop(columns=['total_time_inbed'], inplace=True)\n",
    "    \n",
    "    df_sleepDay = filterDataset(df_sleepDay)\n",
    "    df_sleepDay = reduceMemoryUsage(df_sleepDay)\n",
    "    df_sleepDay = parseDateTime(df_sleepDay)\n",
    "    df_sleepDay.drop(columns=['time'], inplace=True)\n",
    "    \n",
    "    # Write Dataset to csv \n",
    "    interim_data_path = f'{os.getcwd()}/data_interim/'\n",
    "    df_sleepDay.to_csv(f'{interim_data_path}daily_sleep.csv', index=False)\n",
    "    \n",
    "    ## Cleaning Activity Dataset\n",
    "    df_dailyActivity = pd.read_csv(f'{raw_data_path}dailyActivity_merged.csv')\n",
    "    df_dailyActivity = df_dailyActivity.iloc[:,[0,1,2,3,10,11,12,13,14]]\n",
    "    df_dailyActivity.columns = ['id', 'date', 'total_steps', 'total_distance_miles', 'very_active_minutes', 'fairly_active_minutes', 'lightly_active_minutes', 'sedentary_minutes', 'calories']\n",
    "\n",
    "    df_dailyActivity = filterDataset(df_dailyActivity)\n",
    "    df_dailyActivity = reduceMemoryUsage(df_dailyActivity)\n",
    "    \n",
    "    # merge sleep day and activity day\n",
    "    df_merged = pd.merge(df_sleepDay, df_dailyActivity, on=['id', 'date'], how='inner')\n",
    "    df_merged.to_csv(f'{interim_data_path}daily_sleep_activity.csv', index=False)\n",
    "    \n",
    "    print('Daily Activity and Sleep is Merged')\n",
    "    df_merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d181f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter Dataset\n",
      "           id              timestamp  total_sleep_records  \\\n",
      "0  1503960366  4/12/2016 12:00:00 AM                    1   \n",
      "1  1503960366  4/13/2016 12:00:00 AM                    2   \n",
      "2  1503960366  4/15/2016 12:00:00 AM                    1   \n",
      "3  1503960366  4/16/2016 12:00:00 AM                    2   \n",
      "4  1503960366  4/17/2016 12:00:00 AM                    1   \n",
      "\n",
      "   total_minutes_asleep  \n",
      "0                   327  \n",
      "1                   384  \n",
      "2                   412  \n",
      "3                   340  \n",
      "4                   700  \n",
      "Column Names: id                       int64\n",
      "timestamp               object\n",
      "total_sleep_records      int64\n",
      "total_minutes_asleep     int64\n",
      "dtype: object\n",
      "The sum of null values are: id                      0\n",
      "timestamp               0\n",
      "total_sleep_records     0\n",
      "total_minutes_asleep    0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 1652 \n",
      "\n",
      "Count of cells AFTER dropping null: 1652 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 0.04018\n",
      "Memory usage decreased to 0.03506 Mb (12.74177% reduction)\n",
      "------------------------------------------------------------------\n",
      "------------------------------------------------------------------\n",
      "Filter Dataset\n",
      "           id       date  total_steps  total_distance_miles  \\\n",
      "0  1503960366  4/12/2016        13162                  8.50   \n",
      "1  1503960366  4/13/2016        10735                  6.97   \n",
      "2  1503960366  4/14/2016        10460                  6.74   \n",
      "3  1503960366  4/15/2016         9762                  6.28   \n",
      "4  1503960366  4/16/2016        12669                  8.16   \n",
      "\n",
      "   very_active_minutes  fairly_active_minutes  lightly_active_minutes  \\\n",
      "0                   25                     13                     328   \n",
      "1                   21                     19                     217   \n",
      "2                   30                     11                     181   \n",
      "3                   29                     34                     209   \n",
      "4                   36                     10                     221   \n",
      "\n",
      "   sedentary_minutes  calories  \n",
      "0                728      1985  \n",
      "1                776      1797  \n",
      "2               1218      1776  \n",
      "3                726      1745  \n",
      "4                773      1863  \n",
      "Column Names: id                          int64\n",
      "date                       object\n",
      "total_steps                 int64\n",
      "total_distance_miles      float64\n",
      "very_active_minutes         int64\n",
      "fairly_active_minutes       int64\n",
      "lightly_active_minutes      int64\n",
      "sedentary_minutes           int64\n",
      "calories                    int64\n",
      "dtype: object\n",
      "The sum of null values are: id                        0\n",
      "date                      0\n",
      "total_steps               0\n",
      "total_distance_miles      0\n",
      "very_active_minutes       0\n",
      "fairly_active_minutes     0\n",
      "lightly_active_minutes    0\n",
      "sedentary_minutes         0\n",
      "calories                  0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 8460 \n",
      "\n",
      "Count of cells AFTER dropping null: 8460 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 0.11642\n",
      "Memory usage decreased to 0.07966 Mb (31.57205% reduction)\n",
      "------------------------------------------------------------------\n",
      "Daily Activity and Sleep is Merged\n"
     ]
    }
   ],
   "source": [
    "processContextualDatasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bf6fa",
   "metadata": {},
   "source": [
    "Step 3. Generate ManualInputDataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e96bfe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateManualInput():\n",
    "    ids = [150390366, 1927972279, 2873212765, 4319703577, 4558609924, 5577150313, 6962181067, 8877689391]\n",
    "    start_date = datetime(2016, 4, 12)\n",
    "    end_date = datetime(2016, 5, 12)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    data = []\n",
    "    for _id in ids:\n",
    "        initial_kg = random.uniform(50, 120)\n",
    "        initial_bmi = random.uniform(18.5, 30)\n",
    "        initial_fat = random.uniform(15, 35)\n",
    "        direction_two = 0\n",
    "        for date in date_range:\n",
    "            weight_kg = initial_kg + direction_two\n",
    "            weight_pounds = initial_kg * 2.20462\n",
    "            bmi = initial_bmi + direction_two\n",
    "            timestamp = date\n",
    "            fat = initial_fat + direction_two\n",
    "            calorie_consumption = random.randint(1000, 3000)\n",
    "            doctor_visit = random.choice([True, False])\n",
    "            symptom_code = random.choice(['Nausea and Vomiting','Fatigue','None','Headaches','Back Pain','Swelling in Extremities','Heartburn','Constipation','None','Frequent Urination','Braxton Hicks Contractions','Round Ligament Pain', 'None'])\n",
    "            blood_pressure = f\"{str(random.randint(100, 180))}/{str(random.randint(60, 100))}\"\n",
    "            glucose_morning = random.randint(70, 110)\n",
    "            glucose_evening = random.randint(70, 110)\n",
    "            mental_health_code = random.choice( ['None','Anxiety','Depression','None','Stress','Mood Swings','Insomnia','Postpartum Depression','Adjustment Disorder','None','Pregnancy-related OCD','Body Image Issues','Relationship Strain', 'Other', 'None'])\n",
    "\n",
    "            data.append([_id, weight_kg, weight_pounds, bmi, timestamp, fat, calorie_consumption, \n",
    "                         doctor_visit, symptom_code, blood_pressure, glucose_morning, \n",
    "                         glucose_evening, mental_health_code])\n",
    "            \n",
    "            direction_two = random.choice([-2, -1, 0, 1, 2])\n",
    "\n",
    "    df_manual_input = pd.DataFrame(data, columns=['id', 'weight_kg', 'weight_pounds', 'bmi', 'timestamp', \n",
    "                                     'fat', 'calorie_consumption', 'doctor_visit', 'symptom_code', \n",
    "                                     'blood_pressure', 'glucose_morning', 'glucose_evening', \n",
    "                                     'mental_health_code'])\n",
    "\n",
    "    \n",
    "    print(df_manual_input.head())\n",
    "    df_manual_input.to_csv('data_interim/logged_input.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "068ce137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  weight_kg  weight_pounds        bmi  timestamp        fat  \\\n",
      "0  150390366  70.343394     155.080453  29.305366 2016-04-12  20.615125   \n",
      "1  150390366  69.343394     155.080453  28.305366 2016-04-13  19.615125   \n",
      "2  150390366  69.343394     155.080453  28.305366 2016-04-14  19.615125   \n",
      "3  150390366  68.343394     155.080453  27.305366 2016-04-15  18.615125   \n",
      "4  150390366  69.343394     155.080453  28.305366 2016-04-16  19.615125   \n",
      "\n",
      "   calorie_consumption  doctor_visit         symptom_code blood_pressure  \\\n",
      "0                 2849          True                 None         106/78   \n",
      "1                 2642          True                 None         178/66   \n",
      "2                 2458         False                 None         146/79   \n",
      "3                 2332         False                 None         158/72   \n",
      "4                 1044         False  Nausea and Vomiting         104/61   \n",
      "\n",
      "   glucose_morning  glucose_evening     mental_health_code  \n",
      "0              102               75    Adjustment Disorder  \n",
      "1               70              105               Insomnia  \n",
      "2               75               77                   None  \n",
      "3               82              107                  Other  \n",
      "4               86               78  Pregnancy-related OCD  \n"
     ]
    }
   ],
   "source": [
    "generateManualInput()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
