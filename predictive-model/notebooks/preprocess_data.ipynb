{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4efc2fde-43f3-4659-88ec-524310a1cb5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn import preprocessing \n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84a71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Predictive Model Folder: C:\\Users\\ashly\\OneDrive\\Documents\\Education Material\\ResearchProject\\MaternalHealthResearch\\predictive-model\n"
     ]
    }
   ],
   "source": [
    "# set working directory \n",
    "load_dotenv()\n",
    "os.chdir(os.getenv('DEFAULT_PATH'))\n",
    "\n",
    "print('In Predictive Model Folder:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30eb7ac",
   "metadata": {},
   "source": [
    "Step 1: Create the HeartRate and METS merged dataset, filter null values and reduce the memory usage first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a32175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterDataset(df):\n",
    "    print('Filter Dataset')\n",
    "\n",
    "    # view details of the dataset\n",
    "    print(df.head())\n",
    "    print('Column Names:',df.dtypes)\n",
    "\n",
    "    # print null values in the dataframe\n",
    "    print('The sum of null values are:', df.isnull().sum())\n",
    "    \n",
    "    \n",
    "    # drop rows with null values\n",
    "    print('Count of cells BEFORE dropping null:', df.size,'\\n')\n",
    "    df = df.dropna() \n",
    "    print('Count of cells AFTER dropping null:', df.size, '\\n')\n",
    "    print('------------------------------------------------------------------')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed48b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDateTime(df):\n",
    "    # datetime split\n",
    "    split_datetime = df['timestamp'].str.split(' ', expand=True)\n",
    "\n",
    "    # Assign the date and time components to new columns\n",
    "    df['date'] = split_datetime[0]\n",
    "    df['time'] = split_datetime[1]\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['time'] = df['time'].astype(str)\n",
    "\n",
    "    df.drop(columns=['timestamp'], inplace=True)\n",
    "    df[:3]\n",
    "    print('------------------------------------------------------------------')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "227be2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduceMemoryUsage(df, verbose=True):\n",
    "    print('Reduce Memory')\n",
    "    \n",
    "    numerics = {\n",
    "        np.int8: (np.iinfo(np.int8).min,np.iinfo(np.int8).max),\n",
    "        np.int16: (np.iinfo(np.int16).min,np.iinfo(np.int16).max), \n",
    "        np.int32: (np.iinfo(np.int32).min,np.iinfo(np.int32).max), \n",
    "        np.int64: (np.iinfo(np.int64).min,np.iinfo(np.int64).max), \n",
    "        np.float16: (np.finfo(np.float16).min,np.finfo(np.float16).max), \n",
    "        np.float32: (np.finfo(np.float32).min,np.finfo(np.float32).max), \n",
    "        np.float64: (np.finfo(np.float64).min,np.finfo(np.float64).max)\n",
    "        }\n",
    "    types = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_memory_usage = df.memory_usage(deep=True).sum() / (1024 ** 2)\n",
    "    print('Starting memory usage is {:5.5f}'.format(start_memory_usage))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in types: \n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            for n_key, n_value in numerics.items(): \n",
    "                if c_min > n_value[0] and c_max < n_value[1]:\n",
    "                    df[col] = df[col].astype(n_key)\n",
    "                    break\n",
    "    \n",
    "    end_memory_usage = df.memory_usage(deep=True).sum() / (1024**2)\n",
    "    if verbose: \n",
    "        print('Memory usage decreased to {:5.5f} Mb ({:.5f}% reduction)'.format(end_memory_usage, 100 * (start_memory_usage - end_memory_usage) / start_memory_usage))\n",
    "    print('------------------------------------------------------------------')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebe313b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeOutliers(group, target='bpm'):\n",
    "   z_scores = stats.zscore(group[target])\n",
    "   threshold = 3\n",
    "   outlier_indices = group.index[abs(z_scores) > threshold]\n",
    "   return group.drop(outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36738be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeMinMax(df,column_name):\n",
    "    min_value = df[column_name].min()\n",
    "    max_value = df[column_name].max()\n",
    "    x = df[[column_name]].values\n",
    "    x = x.reshape(-1, 1)\n",
    "    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df[column_name] = x_scaled\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9787ef6-d784-4e19-ba4e-402b198477c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHRMetsDataset():\n",
    "    ## Merge cleaned dataframes\n",
    "    \n",
    "    # read the heartrate and mets dataframe\n",
    "    df_mets = pd.read_csv('data_raw/RAW-Fitabase Data 4.12.16-5.12.16/minuteMETsNarrow_merged.csv')\n",
    "    df_heartrate = pd.read_csv('data_raw/RAW-Fitabase Data 4.12.16-5.12.16/heartrate_seconds_merged.csv')\n",
    "    df_intensities = pd.read_csv('data_raw/RAW-Fitabase Data 4.12.16-5.12.16/minuteIntensitiesNarrow_merged.csv')\n",
    "    \n",
    "    # update columns names to be the same\n",
    "    df_mets.columns = ['id', 'timestamp', 'mets']\n",
    "    df_heartrate.columns = ['id', 'timestamp', 'bpm']\n",
    "    df_intensities.columns = ['id','timestamp', 'intensity_level']\n",
    "        \n",
    "    # clean dataframes and reduce memory usage\n",
    "    df_mets = filterDataset(df_mets)\n",
    "    df_mets = reduceMemoryUsage(df_mets)\n",
    "    # df_mets = normalizeMinMax(df_mets, 'mets')\n",
    "    \n",
    "    df_heartrate = filterDataset(df_heartrate)\n",
    "    df_heartrate = reduceMemoryUsage(df_heartrate)\n",
    "   \n",
    "    df_intensities = filterDataset(df_intensities) \n",
    "    df_intensities = reduceMemoryUsage(df_intensities) \n",
    "    \n",
    "    # merge dataframes using the column names and with an inner join\n",
    "    df_merged_outer = pd.merge(df_mets, df_heartrate, on=['id', 'timestamp'], how='outer')\n",
    "    df_merged_inner = pd.merge(df_mets, df_heartrate, on=['id', 'timestamp'], how='inner')\n",
    "    df_merged_right = pd.merge(df_mets, df_heartrate, on=['id', 'timestamp'], how='right')\n",
    "    \n",
    "    df_merged_outer = pd.merge(df_intensities, df_merged_outer, on=['id', 'timestamp'], how='outer')\n",
    "    df_merged_inner = pd.merge(df_intensities, df_merged_inner, on=['id', 'timestamp'], how='inner')\n",
    "    df_merged_right = pd.merge(df_intensities, df_merged_right, on=['id', 'timestamp'], how='right')\n",
    "    \n",
    "    # remove outliers \n",
    "    g1 = df_merged_outer.groupby('intensity_level')\n",
    "    g2 = df_merged_inner.groupby('intensity_level')\n",
    "    g3 = df_merged_right.groupby('intensity_level')\n",
    "    \n",
    "    df_merged_outer = g1.apply(removeOutliers)\n",
    "    df_merged_inner = g2.apply(removeOutliers) \n",
    "    df_merged_right = g3.apply(removeOutliers)\n",
    "   \n",
    "    df_merged_inner.to_csv('data_interim/heartrate_mets_intensities_merged_outer.csv', index=False)\n",
    "    df_merged_inner.to_csv('data_interim/heartrate_mets_intensities_merged_inner.csv', index=False)\n",
    "    df_merged_right.to_csv('data_interim/heartrate_mets_intensities_merged_right.csv', index=False)\n",
    "    \n",
    "    # create heartrate dataset of cleaned results\n",
    "    df_heartrate.to_csv('data_interim/heartrate_fiveseconds_intervals.csv', index=False)\n",
    "   \n",
    "    print('Heartrate, METS, and minuteIntensity dataframes are merged')\n",
    "    df_merged_inner[:3]\n",
    "    df_merged_right[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d87cf5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filter Dataset\n",
      "           id              timestamp  mets\n",
      "0  1503960366  4/12/2016 12:00:00 AM    10\n",
      "1  1503960366  4/12/2016 12:01:00 AM    10\n",
      "2  1503960366  4/12/2016 12:02:00 AM    10\n",
      "3  1503960366  4/12/2016 12:03:00 AM    10\n",
      "4  1503960366  4/12/2016 12:04:00 AM    10\n",
      "Column Names: id            int64\n",
      "timestamp    object\n",
      "mets          int64\n",
      "dtype: object\n",
      "The sum of null values are: id           0\n",
      "timestamp    0\n",
      "mets         0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 3976740 \n",
      "\n",
      "Count of cells AFTER dropping null: 3976740 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 107.41986\n",
      "Memory usage decreased to 99.83484 Mb (7.06111% reduction)\n",
      "------------------------------------------------------------------\n",
      "Filter Dataset\n",
      "           id             timestamp  bpm\n",
      "0  2022484408  4/12/2016 7:21:00 AM   97\n",
      "1  2022484408  4/12/2016 7:21:05 AM  102\n",
      "2  2022484408  4/12/2016 7:21:10 AM  105\n",
      "3  2022484408  4/12/2016 7:21:20 AM  103\n",
      "4  2022484408  4/12/2016 7:21:25 AM  101\n",
      "Column Names: id            int64\n",
      "timestamp    object\n",
      "bpm           int64\n",
      "dtype: object\n",
      "The sum of null values are: id           0\n",
      "timestamp    0\n",
      "bpm          0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 7450974 \n",
      "\n",
      "Count of cells AFTER dropping null: 7450974 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 201.22400\n",
      "Memory usage decreased to 187.01239 Mb (7.06258% reduction)\n",
      "------------------------------------------------------------------\n",
      "Filter Dataset\n",
      "           id              timestamp  intensity_level\n",
      "0  1503960366  4/12/2016 12:00:00 AM                0\n",
      "1  1503960366  4/12/2016 12:01:00 AM                0\n",
      "2  1503960366  4/12/2016 12:02:00 AM                0\n",
      "3  1503960366  4/12/2016 12:03:00 AM                0\n",
      "4  1503960366  4/12/2016 12:04:00 AM                0\n",
      "Column Names: id                  int64\n",
      "timestamp          object\n",
      "intensity_level     int64\n",
      "dtype: object\n",
      "The sum of null values are: id                 0\n",
      "timestamp          0\n",
      "intensity_level    0\n",
      "dtype: int64\n",
      "Count of cells BEFORE dropping null: 3976740 \n",
      "\n",
      "Count of cells AFTER dropping null: 3976740 \n",
      "\n",
      "------------------------------------------------------------------\n",
      "Reduce Memory\n",
      "Starting memory usage is 107.41986\n",
      "Memory usage decreased to 98.57066 Mb (8.23796% reduction)\n",
      "------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashly\\AppData\\Local\\Temp\\ipykernel_40404\\3070593689.py:39: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_merged_outer = g1.apply(removeOutliers)\n",
      "C:\\Users\\ashly\\AppData\\Local\\Temp\\ipykernel_40404\\3070593689.py:40: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_merged_inner = g2.apply(removeOutliers)\n",
      "C:\\Users\\ashly\\AppData\\Local\\Temp\\ipykernel_40404\\3070593689.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_merged_right = g3.apply(removeOutliers)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heartrate, METS, and minuteIntensity dataframes are merged\n"
     ]
    }
   ],
   "source": [
    "createHRMetsDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc439b1f",
   "metadata": {},
   "source": [
    "Step 2: Sleep & Actvity Tracking Dataset Cleaning (Select the most important attributes and convert each dataset into interim ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a626bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processContextualDatasets():\n",
    "    raw_data_path = f'{os.getcwd()}/data_raw/RAW-Fitabase Data 4.12.16-5.12.16/'\n",
    "    \n",
    "    ## Cleaning SleepDay Dataset\n",
    "    df_sleepDay = pd.read_csv(raw_data_path+'sleepDay_merged.csv')\n",
    "    df_sleepDay.columns = ['id', 'timestamp', 'total_sleep_records', 'total_minutes_asleep', 'total_time_inbed']\n",
    "    df_sleepDay.drop(columns=['total_time_inbed'], inplace=True)\n",
    "    \n",
    "    df_sleepDay = filterDataset(df_sleepDay)\n",
    "    df_sleepDay = reduceMemoryUsage(df_sleepDay)\n",
    "    df_sleepDay = parseDateTime(df_sleepDay)\n",
    "    df_sleepDay.drop(columns=['time'], inplace=True)\n",
    "    \n",
    "    # Write Dataset to csv \n",
    "    interim_data_path = f'{os.getcwd()}/data_interim/'\n",
    "    df_sleepDay.to_csv(f'{interim_data_path}daily_sleep.csv', index=False)\n",
    "    \n",
    "    ## Cleaning Activity Dataset\n",
    "    df_dailyActivity = pd.read_csv(f'{raw_data_path}dailyActivity_merged.csv')\n",
    "    df_dailyActivity = df_dailyActivity.iloc[:,[0,1,2,3,10,11,12,13,14]]\n",
    "    df_dailyActivity.columns = ['id', 'date', 'total_steps', 'total_distance_miles', 'very_active_minutes', 'fairly_active_minutes', 'lightly_active_minutes', 'sedentary_minutes', 'calories']\n",
    "\n",
    "    df_dailyActivity = filterDataset(df_dailyActivity)\n",
    "    df_dailyActivity = reduceMemoryUsage(df_dailyActivity)\n",
    "    \n",
    "    # merge sleep day and activity day\n",
    "    df_merged = pd.merge(df_sleepDay, df_dailyActivity, on=['id', 'date'], how='inner')\n",
    "    df_merged.to_csv(f'{interim_data_path}daily_sleep_activity.csv', index=False)\n",
    "    \n",
    "    print('Daily Activity and Sleep is Merged')\n",
    "    df_merged[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d181f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processContextualDatasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73bf6fa",
   "metadata": {},
   "source": [
    "Step 3. Generate ManualInputDataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e96bfe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateManualInput():\n",
    "    ids = [150390366, 1927972279, 2873212765, 4319703577, 4558609924, 5577150313, 6962181067, 8877689391]\n",
    "    start_date = datetime(2016, 4, 12)\n",
    "    end_date = datetime(2016, 5, 12)\n",
    "    date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "    data = []\n",
    "    for _id in ids:\n",
    "        initial_kg = random.uniform(50, 120)\n",
    "        initial_bmi = random.uniform(18.5, 30)\n",
    "        initial_fat = random.uniform(15, 35)\n",
    "        direction_two = 0\n",
    "        for date in date_range:\n",
    "            weight_kg = initial_kg + direction_two\n",
    "            weight_pounds = initial_kg * 2.20462\n",
    "            bmi = initial_bmi + direction_two\n",
    "            timestamp = date\n",
    "            fat = initial_fat + direction_two\n",
    "            calorie_consumption = random.randint(1000, 3000)\n",
    "            doctor_visit = random.choice([True, False])\n",
    "            symptom_code = random.choice(['Nausea and Vomiting','Fatigue','None','Headaches','Back Pain','Swelling in Extremities','Heartburn','Constipation','None','Frequent Urination','Braxton Hicks Contractions','Round Ligament Pain', 'None'])\n",
    "            systolic_bp = random.randint(100, 180)\n",
    "            diastolic_bp = random.randint(60, 100)\n",
    "            glucose_morning = random.randint(70, 110)\n",
    "            glucose_evening = random.randint(70, 110)\n",
    "            mental_health_code = random.choice( ['None','Anxiety','Depression','None','Stress','Mood Swings','Insomnia','Postpartum Depression','Adjustment Disorder','None','Pregnancy-related OCD','Body Image Issues','Relationship Strain', 'Other', 'None'])\n",
    "\n",
    "            data.append([_id, weight_kg, weight_pounds, bmi, timestamp, fat, calorie_consumption, \n",
    "                         doctor_visit, symptom_code, systolic_bp, diastolic_bp, glucose_morning, \n",
    "                         glucose_evening, mental_health_code])\n",
    "            \n",
    "            direction_two = random.choice([-2, -1, 0, 1, 2])\n",
    "\n",
    "    df_manual_input = pd.DataFrame(data, columns=['id', 'weight_kg', 'weight_pounds', 'bmi', 'timestamp', \n",
    "                                     'fat', 'calorie_consumption', 'doctor_visit', 'symptom_code', \n",
    "                                     'systolic_bp', 'diastolic_bp','glucose_morning', 'glucose_evening', \n",
    "                                     'mental_health_code'])\n",
    "\n",
    "    \n",
    "    print(df_manual_input.head())\n",
    "    df_manual_input.to_csv('data_interim/logged_input.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "068ce137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id  weight_kg  weight_pounds        bmi  timestamp        fat  \\\n",
      "0  150390366  82.430283     181.727451  23.681953 2016-04-12  27.944216   \n",
      "1  150390366  80.430283     181.727451  21.681953 2016-04-13  25.944216   \n",
      "2  150390366  83.430283     181.727451  24.681953 2016-04-14  28.944216   \n",
      "3  150390366  84.430283     181.727451  25.681953 2016-04-15  29.944216   \n",
      "4  150390366  84.430283     181.727451  25.681953 2016-04-16  29.944216   \n",
      "\n",
      "   calorie_consumption  doctor_visit                symptom_code  systolic_bp  \\\n",
      "0                 1938          True  Braxton Hicks Contractions          174   \n",
      "1                 1807          True  Braxton Hicks Contractions          105   \n",
      "2                 1612          True                Constipation          170   \n",
      "3                 2774         False          Frequent Urination          112   \n",
      "4                 2531         False                   Heartburn          109   \n",
      "\n",
      "   diastolic_bp  glucose_morning  glucose_evening     mental_health_code  \n",
      "0            89               88              100      Body Image Issues  \n",
      "1            68               84               77                   None  \n",
      "2            91               82               82             Depression  \n",
      "3            68              103              102    Adjustment Disorder  \n",
      "4            69               82               85  Pregnancy-related OCD  \n"
     ]
    }
   ],
   "source": [
    "generateManualInput()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
